# ConsolPortals Development Guide

**Welcome!** This guide will walk you through running the ConsolPortals application stack locally and deploying to Kubernetes.

## ğŸ“‹ Table of Contents

1. [What is ConsolPortals?](#what-is-consolportals)
2. [Architecture Overview](#architecture-overview)
3. [Prerequisites](#prerequisites)
4. [Quick Start - Choose Your Path](#quick-start---choose-your-path)
5. [Deployment Paths](#deployment-paths)
   - [Path 1: Docker Compose (Fastest)](#path-1-docker-compose-fastest)
   - [Path 2: Local Kubernetes](#path-2-local-kubernetes)
   - [Path 3: Corporate Kubernetes Cluster](#path-3-corporate-kubernetes-cluster)
6. [Next Steps](#next-steps)

---

## What is ConsolPortals?

ConsolPortals is a multi-portal frontend environment that includes:
- **Admin Portal** - Administrative interface
- **CC Portal** - Customer care interface  
- **Partner Portal** - Partner management interface
- **httpd** - Apache web server (Layer 1 reverse proxy)
- **a3gw** - API Gateway (Layer 2 reverse proxy)

### Traffic Flow
```
User â†’ httpd:80/443 â†’ a3gw:8444/8445 â†’ portals (8080-8082) â†’ Backend APIs
```

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Browser (User)                                             â”‚
â”‚  https://hostname/adminportal                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  httpd (Apache Web Server) - Port 80/443                    â”‚
â”‚  â€¢ Serves static assets (HTML, JS, CSS)                     â”‚
â”‚  â€¢ First layer reverse proxy                                â”‚
â”‚  â€¢ Enforces security headers (CSP, X-Frame-Options)         â”‚
â”‚  â€¢ Session management (with sticky sessions)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  a3gw (API Gateway) - Port 8444/8445                        â”‚
â”‚  â€¢ Second layer reverse proxy                               â”‚
â”‚  â€¢ Routes to portals and backend APIs                       â”‚
â”‚  â€¢ Session management ? (stateless nodejs proxy server)     â”‚
â”‚    Since A3GW is stateless by nature, httpd should fwd      â”‚
â”‚    incoming requests to the same pod in K8S/helm setups     â”‚
â”‚    otherwise audit logging won't make sense,                â”‚
â”‚    and captcha might not work                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                 â–¼            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Admin Portal  â”‚  â”‚CC Portal â”‚  â”‚Partner Port â”‚
        â”‚   :8080       â”‚  â”‚  :8081   â”‚  â”‚    :8082    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Backend APIs   â”‚
              â”‚  (External)     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Prerequisites

### All Deployment Paths
- **Git** - For cloning the repository
- **Basic understanding** of Docker and Kubernetes concepts

### For Docker Compose (Path 1)
- Docker Desktop (Mac/Windows) or Docker Engine (Linux)
- Docker Compose v2+
- **Ports required:** 80, 443, 8080-8082, 8444-8445

### For Local Kubernetes (Path 2)
- Docker Desktop with Kubernetes enabled (recommended for Mac)
  - OR Minikube
  - OR Kind
- kubectl CLI tool
- Helm 3.x
- **Ports required:** Depends on configuration (see [Local Dev Guide](README-LocalDevGuide.md))

### For Corporate Kubernetes (Path 3)
- Access to corporate Kubernetes cluster
- kubectl configured with cluster credentials
- Helm 3.x
- Access to Nexus registry: `nexus.telenity.com`
- Docker (for building and pushing images)

---

## Quick Start - Choose Your Path

### ğŸš€ I want to run it locally as fast as possible
â†’ **Go to [Path 1: Docker Compose](#path-1-docker-compose-fastest)**

### ğŸ¯ I want to test Kubernetes deployment locally
â†’ **Go to [Path 2: Local Kubernetes](#path-2-local-kubernetes)**

### ğŸ¢ I want to deploy to our corporate cluster
â†’ **Go to [Path 3: Corporate Kubernetes Cluster](#path-3-corporate-kubernetes-cluster)**

---

## Deployment Paths

## Path 1: Docker Compose (Fastest)

**Best for:** Quick local development, hot-reloading, rapid iteration

### Step 1: Clone the Repository
```bash
git clone <repository-url>
cd consolportals
```

### Step 2: Choose Your Environment

We have two Docker Compose configurations:

#### Development Environment (with hot-reloading)
```bash
# Using Make (recommended)
make start dev

# Or directly with docker-compose
docker compose -f docker-compose_dev.yml up --build
```

**Features:**
- âœ… Hot-reloading for portal changes
- âœ… Source code mounted as volumes
- âœ… Runs on standard ports (80, 443)

#### Production Environment (static builds)
```bash
# Using Make (recommended)
make start prod

# Or directly with docker-compose
docker compose -f docker-compose_prod.yml up --build
```

**Features:**
- âœ… Production-like builds
- âœ… No volume mounts (faster)
- âœ… Runs on ports 9080, 9443

### Step 3: Access the Portals

**For DEV environment:**
- Admin Portal: http://localhost/adminportal
- CC Portal: http://localhost/ccportal
- Partner Portal: http://localhost/partnerportal

**For PROD environment:**
- Admin Portal: http://localhost:9080/adminportal
- CC Portal: http://localhost:9080/ccportal
- Partner Portal: http://localhost:9080/partnerportal

### Step 4: Stop the Environment
```bash
# Using Make
make stop dev

# Or directly
docker compose -f docker-compose_dev.yml down
```

### Additional Docker Commands

For a complete list of Docker Compose commands and operations, see **[README-Make.md](README-Make.md)**

**Useful commands:**
```bash
make logs dev          # View logs
make restart dev       # Restart containers
make shell dev         # Open shell in container
make rebuild dev       # Rebuild from scratch
make fresh dev         # Complete reset
```

### Troubleshooting Docker

**Port already in use:**
```bash
# Find what's using port 80
lsof -ti:80 | xargs kill

# Or use different ports in docker-compose_prod.yml
```

**Need to rebuild everything:**
```bash
make fresh dev
```

---

## Path 2: Local Kubernetes

**Best for:** Testing Kubernetes deployments, learning Helm, production-like environment locally

### Step 1: Enable Kubernetes

**On Docker Desktop (Mac/Windows):**
1. Open Docker Desktop
2. Go to Settings â†’ Kubernetes
3. Enable "Enable Kubernetes"
4. Click "Apply & Restart"
5. Wait for Kubernetes to start (green indicator)

**Verify Kubernetes is running:**
```bash
kubectl cluster-info
kubectl get nodes
```

### Step 2: Install NGINX Ingress Controller (Optional)

Only needed if you want to use Ingress (Option 3 below):

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml

# Verify
kubectl get pods -n ingress-nginx
```

### Step 3: Build and Push Images (First Time Only)

If you haven't built Docker images yet:

```bash
# Development images
docker compose -f docker-compose_dev.yml build

# Production images  
docker compose -f docker-compose_prod.yml build
```

### Step 4: Choose Your Local K8s Setup

You have **THREE options** for running Kubernetes locally. Choose based on your needs:

#### â­ **Option 1: NodePort (Easiest - Recommended for Mac)**

**Best for:** Daily development, no port-forwarding hassle

```bash
# Create namespace
kubectl create namespace stc-vcp-services

# Install with NodePort
helm install consolportals . \
  -f values.yaml \
  -f values-local-nodeport.yaml \
  -n stc-vcp-services

# Access immediately
open http://localhost:30080/adminportal
```

**Access URLs:**
- Admin Portal: http://localhost:30080/adminportal
- CC Portal: http://localhost:30080/ccportal
- Partner Portal: http://localhost:30080/partnerportal
- HTTPS: https://localhost:30443/adminportal
- Direct a3gw: http://localhost:30444/adminportal

**Pros:**
- âœ… No port-forwarding needed
- âœ… Services stay running
- âœ… Fastest to get started

**Cons:**
- âš ï¸ Different ports than production (30080 vs 80)

---

#### **Option 2: ClusterIP + Port Forwarding (Most Flexible)**

**Best for:** Debugging specific services, custom port selection

```bash
# Create namespace
kubectl create namespace stc-vcp-services

# Install with ClusterIP
helm install consolportals . \
  -f values.yaml \
  -f values-local.yaml \
  -n stc-vcp-services

# Start port forwarding
kubectl port-forward svc/consolportals-sa-stc-vcp-httpd-service 9080:80 9443:443 -n stc-vcp-services &
kubectl port-forward svc/consolportals-sa-stc-vcp-a3gw-service 9444:8444 9445:8445 -n stc-vcp-services &
```

**Access URLs:**
- Admin Portal: http://localhost:9080/adminportal
- Direct a3gw: http://localhost:9444/adminportal

**Managing Port Forwards:**
```bash
# Stop all port forwards
pkill -f "kubectl port-forward"

# Check running port forwards
ps aux | grep "kubectl port-forward"
```

**Pros:**
- âœ… Choose your own ports
- âœ… Matches production architecture
- âœ… More secure

**Cons:**
- âš ï¸ Must manually start port-forwards
- âš ï¸ Stops when terminal closes

---

#### **Option 3: ClusterIP + Ingress (Most Production-Like)**

**Best for:** Testing full production setup, SSL termination

**Prerequisites:** NGINX Ingress Controller must be installed (see Step 2)

```bash
# Create namespace
kubectl create namespace stc-vcp-services

# Install with Ingress enabled
helm install consolportals . \
  -f values.yaml \
  -f values-local.yaml \
  -n stc-vcp-services
```

**Access URLs:**
- Admin Portal: http://localhost/adminportal
- CC Portal: http://localhost/ccportal  
- Partner Portal: http://localhost/partnerportal

**Pros:**
- âœ… Exact production setup
- âœ… Clean URLs
- âœ… SSL termination at ingress

**Cons:**
- âš ï¸ Requires Ingress Controller
- âš ï¸ Extra component to manage

---

### Step 5: Verify Deployment

```bash
# Check pod status
kubectl get pods -n stc-vcp-services

# Check services
kubectl get services -n stc-vcp-services

# Check ingress (if using Option 3)
kubectl get ingress -n stc-vcp-services

# View logs
kubectl logs -f deployment/consolportals-sa-stc-vcp-httpd-deployment -n stc-vcp-services
```

### Step 6: Making Changes

If you update code or configuration:

```bash
# Rebuild Docker images
docker compose -f docker-compose_dev.yml build

# Upgrade Helm deployment
helm upgrade consolportals . -f values-local-nodeport.yaml -n stc-vcp-services

# Or restart a specific pod
kubectl rollout restart deployment/consolportals-sa-stc-vcp-a3gw-deployment -n stc-vcp-services
```

### Cleanup

```bash
# Uninstall
helm uninstall consolportals -n stc-vcp-services

# Delete namespace
kubectl delete namespace stc-vcp-services
```

### Detailed Local K8s Guide

For more details on local Kubernetes options, see **[README-LocalDevGuide.md](README-LocalDevGuide.md)**

---

## Path 3: Corporate Kubernetes Cluster

**Best for:** Deploying to shared development/staging environment

### Overview

The corporate deployment uses:
- **Namespace:** `stc-vcp-services` (or your custom namespace)
- **Registry:** `nexus.telenity.com`
- **Architecture:** AMD64 (Linux servers)
- **Image tags:** Must include `-amd64` suffix

### Step 1: Configure kubectl

Make sure you have access to the corporate cluster:

```bash
# Test connection
kubectl cluster-info

# Check available namespaces
kubectl get namespaces

# Switch to your namespace
kubectl config set-context --current --namespace=stc-vcp-services
```

### Step 2: Build AMD64 Images

**Important:** Corporate cluster runs on AMD64 architecture. If you're on Mac (ARM), you must build multi-platform images.

```bash
# Build AMD64 images for each component
docker buildx build --platform linux/amd64 \
  -t nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-httpd:1.0.0.1-amd64 \
  -f httpd/Dockerfile.vcp.prod \
  ./httpd

docker buildx build --platform linux/amd64 \
  -t nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-a3gw:1.0.0.1-amd64 \
  -f a3gw/Dockerfile.vcp.prod \
  ./a3gw

docker buildx build --platform linux/amd64 \
  -t nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-adminportal:1.0.0.1-amd64 \
  -f vcp-adminportal/Dockerfile.prod \
  ./vcp-adminportal

# Similar for ccportal and partnerportal
```

**Note:** Update `1.0.0.1` to your actual version number.

### Step 3: Push Images to Nexus Registry

```bash
# Login to Nexus (if not already logged in)
docker login nexus.telenity.com

# Push all images
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-httpd:1.0.0.1-amd64
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-a3gw:1.0.0.1-amd64
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-adminportal:1.0.0.1-amd64
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-ccportal:1.0.0.1-amd64
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-partnerportal:1.0.0.1-amd64
```

### Step 4: Create Namespace (First Time Only)

```bash
# Create your namespace if it doesn't exist
kubectl create namespace stc-vcp-services

# Verify
kubectl get namespace stc-vcp-services
```

### Step 5: Deploy with Helm

```bash
# Deploy using production values
helm install consolportals . \
  -f values.yaml \
  -n stc-vcp-services

# Or with dev environment values
helm install consolportals . \
  -f values.yaml \
  -f values-dev.yaml \
  -n stc-vcp-services
```

**The `values.yaml` file includes:**
- Registry: `nexus.telenity.com`
- Image tags: `1.0.0.1-amd64`
- Replicas: 2 per service
- Session affinity: Enabled (3 hours)
- Host: `consolportals.internal.telenity.com`

### Step 6: Verify Deployment

```bash
# Check pod status
kubectl get pods -n stc-vcp-services

# Check services
kubectl get services -n stc-vcp-services

# Check ingress
kubectl get ingress -n stc-vcp-services

# View events
kubectl get events -n stc-vcp-services --sort-by='.lastTimestamp'
```

### Step 7: Access the Application

**Access depends on your cluster's ingress configuration:**

```bash
# If using ingress with hostname
https://consolportals.internal.telenity.com/adminportal

# Or via port-forwarding for testing
kubectl port-forward svc/consolportals-sa-stc-vcp-httpd-service 9080:80 -n stc-vcp-services
http://localhost:9080/adminportal
```

### Updating Deployment

When you make changes:

```bash
# 1. Rebuild and push new images (update version tag!)
docker buildx build --platform linux/amd64 \
  -t nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-httpd:1.0.0.2-amd64 \
  ./httpd
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-httpd:1.0.0.2-amd64

# 2. Update values.yaml with new tag
# Edit values.yaml: change tag from 1.0.0.1-amd64 to 1.0.0.2-amd64

# 3. Upgrade Helm deployment
helm upgrade consolportals . -f values.yaml -n stc-vcp-services
```

### Cleanup

```bash
# Uninstall
helm uninstall consolportals -n stc-vcp-services

# Delete namespace (careful!)
kubectl delete namespace stc-vcp-services
```

### Troubleshooting Corporate Cluster

**Images won't pull:**
```bash
# Check if image exists in registry
docker pull nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-httpd:1.0.0.1-amd64

# Check pod events
kubectl describe pod <pod-name> -n stc-vcp-services
```

**Architecture mismatch:**
```bash
# Verify image architecture
docker inspect nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-httpd:1.0.0.1-amd64 | grep Architecture
# Should show: "Architecture": "amd64"
```

**Pods not starting:**
```bash
# Check logs
kubectl logs deployment/consolportals-sa-stc-vcp-httpd-deployment -n stc-vcp-services

# Check events
kubectl get events -n stc-vcp-services --sort-by='.lastTimestamp'
```

---

## Next Steps

### ğŸ“š Additional Documentation

- **[README-Helm.md](README-Helm.md)** - Comprehensive Helm chart documentation
- **[README-LocalDevGuide.md](README-LocalDevGuide.md)** - Detailed local K8s options
- **[README-Make.md](README-Make.md)** - Complete Makefile command reference
- **[README-Dockerization.md](README-Dockerization.md)** - Docker implementation details
- **[README-Ingress.md](README-Ingress.md)** - Ingress configuration guide
- **[README-Environment.md](README-Environment.md)** - Environment-specific configurations

### ğŸ”§ Configuration Files

- `values.yaml` - Base Helm values (production)
- `values-local.yaml` - Local development (Mac, no architecture suffix)
- `values-local-nodeport.yaml` - Local with NodePort
- `values-dev.yaml` - Development environment
- `docker-compose_dev.yml` - Docker Compose dev setup
- `docker-compose_prod.yml` - Docker Compose production

### ğŸ“ Project Structure

```
consolportals/
â”œâ”€â”€ a3gw/                    # API Gateway
â”œâ”€â”€ httpd/                   # Apache web server
â”œâ”€â”€ vcp-adminportal/         # Admin portal (AngularJS)
â”œâ”€â”€ vcp-ccportal/            # CC portal
â”œâ”€â”€ vcp-partnerportal/       # Partner portal
â”œâ”€â”€ templates/               # Helm templates
â”‚   â”œâ”€â”€ a3gw-deployment.yaml
â”‚   â”œâ”€â”€ httpd-deployment.yaml
â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â””â”€â”€ ...
â”œâ”€â”€ values.yaml              # Production Helm values
â”œâ”€â”€ values-local*.yaml       # Local variants
â”œâ”€â”€ docker-compose_*.yml     # Docker Compose files
â”œâ”€â”€ Makefile                 # Automation commands
â””â”€â”€ README*.md               # Documentation
```

### ğŸ› Common Issues

**Docker:**
- Port conflicts â†’ See [README-Make.md](README-Make.md)
- Volume issues â†’ Try `make fresh dev`

**Kubernetes:**
- Image pull errors â†’ Check registry access and image tags
- Pod crashes â†’ Check logs with `kubectl logs`
- Port conflicts â†’ Use different NodePort values

**Corporate Cluster:**
- Image architecture â†’ Must use AMD64 images with `-amd64` suffix
- Registry access â†’ Ensure `docker login nexus.telenity.com` works
- Namespace access â†’ Verify with `kubectl get pods -n stc-vcp-services`

### ğŸ’¡ Tips

1. **Start with Docker Compose** to understand the application
2. **Move to Local K8s** when you want to test deployment
3. **Deploy to Corporate Cluster** when ready for integration
4. **Use NodePort locally** for easiest Mac development
5. **Enable hot-reloading** in dev with Docker Compose
6. **Version your images** properly (increment tags)
7. **Check logs first** when debugging

---

## ğŸ†˜ Getting Help

1. Check the detailed README files in the links above
2. Review the troubleshooting sections
3. Check pod logs: `kubectl logs -f <pod-name> -n stc-vcp-services`
4. View events: `kubectl get events -n stc-vcp-services`
5. Describe failing resources: `kubectl describe pod <pod-name> -n stc-vcp-services`

---

**Last Updated:** January 2026
