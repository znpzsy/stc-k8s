# Developer Journey - From Docker to Production

This guide walks you through the typical progression of deploying ConsolPortals, from local development to production deployment.

## üìç The Journey Overview

```
Stage 1: Docker Compose (Local Dev)
    ‚Üì
Stage 2: Local Kubernetes (Learning & Testing)
    ‚Üì
Stage 3: Corporate Kubernetes (Deployment)
```

Each stage builds on the previous one, adding complexity but getting closer to production.

---

## Stage 1: Docker Compose (Local Development)

### Why Start Here?

- **Fastest way** to get the application running
- **Hot-reloading** for rapid development
- **Simple debugging** with docker logs
- **No Kubernetes knowledge** required

### What You Learn

- How the application components work together
- The basic architecture (httpd ‚Üí a3gw ‚Üí portals)
- Configuration file structure
- Port mappings and service dependencies

### Setup

```bash
# 1. Clone and enter directory
git clone <repository-url>
cd consolportals

# 2. Start development environment
make start dev

# 3. Access the portals
open http://localhost/adminportal
```

### Development Workflow

```bash
# Make code changes (automatically reflected)
vim vcp-adminportal/src/app/main.js

# View logs
make logs dev

# Restart if needed
make restart dev

# Complete rebuild
make rebuild dev
```

### Key Files at This Stage

- `docker-compose_dev.yml` - Development configuration with hot-reload
- `docker-compose_prod.yml` - Production-like builds
- `Makefile` - Convenience commands
- `Dockerfile.*` files - Build instructions for each service

### When to Move to Stage 2

You should move to Kubernetes when:
- ‚úÖ You understand how the components interact
- ‚úÖ You want to test container orchestration
- ‚úÖ You need to simulate production deployment
- ‚úÖ You want to learn Kubernetes concepts

---

## Stage 2: Local Kubernetes (Testing & Learning)

### Why Move to Kubernetes?

- **Production-like environment** on your machine
- **Learn Kubernetes** concepts safely
- **Test deployment configurations** before production
- **Experience with Helm** charts

### The Transition

#### Step 1: Enable Kubernetes

**On Docker Desktop:**
1. Settings ‚Üí Kubernetes ‚Üí Enable Kubernetes
2. Wait for Kubernetes to start

**Verify:**
```bash
kubectl cluster-info
kubectl get nodes
```

#### Step 2: Understand the Changes

**Docker Compose (what you had):**
```yaml
services:
  httpd:
    build: ./httpd
    ports:
      - '80:80'
    networks:
      - vcp-network
```

**Kubernetes (what you're moving to):**
```yaml
# Deployment - How to run the container
apiVersion: apps/v1
kind: Deployment
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: httpd
        image: stc-vcp/httpd:latest
        ports:
        - containerPort: 80

---
# Service - How to expose it
apiVersion: v1
kind: Service
spec:
  type: ClusterIP  # or NodePort
  ports:
  - port: 80
    targetPort: 80
```

**Helm Chart (what makes it easier):**
```yaml
# values.yaml - Configuration
httpd:
  image:
    repository: stc-vcp/httpd
    tag: latest
  replicaCount: 1
```

#### Step 3: Choose Your K8s Method

You have three options. I recommend this progression:

**Week 1-2: Start with NodePort** (Easiest)
```bash
helm install consolportals . -f values-local-nodeport.yaml -n stc-vcp-services
open http://localhost:30080/adminportal
```
- ‚úÖ No port-forwarding hassle
- ‚úÖ Services always accessible
- ‚úÖ Great for daily development

**Week 3-4: Try ClusterIP + Port Forward** (More Production-Like)
```bash
helm install consolportals . -f values-local.yaml -n stc-vcp-services
kubectl port-forward svc/consolportals-sa-stc-vcp-httpd-service 9080:80 -n stc-vcp-services
```
- ‚úÖ Closer to production architecture
- ‚úÖ Learn port-forwarding
- ‚úÖ More secure

**Week 5+: Use Ingress** (Most Production-Like)
```bash
# Install NGINX Ingress Controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml

# Deploy application
helm install consolportals . -f values-local.yaml -n stc-vcp-services
open http://localhost/adminportal
```
- ‚úÖ Exact production setup
- ‚úÖ Clean URLs
- ‚úÖ SSL termination

### Development Workflow in Kubernetes

```bash
# Make code changes
vim vcp-adminportal/src/app/main.js

# Rebuild Docker image
docker compose -f docker-compose_dev.yml build vcpadminportal

# Upgrade Helm deployment
helm upgrade consolportals . -f values-local-nodeport.yaml -n stc-vcp-services

# Or force pod restart
kubectl rollout restart deployment/consolportals-sa-stc-vcp-adminportal-deployment -n stc-vcp-services
```

### Key Files at This Stage

- `values.yaml` - Base Helm configuration
- `values-local.yaml` - Your local overrides
- `values-local-nodeport.yaml` - NodePort configuration
- `templates/*.yaml` - Kubernetes manifests
- `Chart.yaml` - Helm chart metadata

### What You Learn in Stage 2

- **Kubernetes Concepts:**
  - Pods, Deployments, Services
  - Namespaces
  - ConfigMaps and Secrets
  - Ingress and routing
  
- **Helm Concepts:**
  - Charts and templates
  - Values files and overrides
  - Upgrades and rollbacks
  
- **Debugging Skills:**
  - `kubectl logs`
  - `kubectl describe`
  - `kubectl get events`
  - Port forwarding

### Common Issues and Solutions

**Issue: Pods won't start**
```bash
# Check status
kubectl get pods -n stc-vcp-services

# View events
kubectl describe pod <pod-name> -n stc-vcp-services

# Check logs
kubectl logs <pod-name> -n stc-vcp-services
```

**Issue: Can't pull images**
```bash
# Make sure images exist
docker images | grep stc-vcp

# Or build them
docker compose -f docker-compose_dev.yml build
```

**Issue: Port conflicts**
```bash
# With NodePort, change ports in values-local-nodeport.yaml
# With ClusterIP, change port-forward ports
# With Ingress, conflicts are rare (uses 80/443)
```

### When to Move to Stage 3

Move to corporate Kubernetes when:
- ‚úÖ You're comfortable with Kubernetes basics
- ‚úÖ You understand Helm charts
- ‚úÖ You need to share your work with team
- ‚úÖ You're ready for integration testing
- ‚úÖ You need to deploy to a real environment

---

## Stage 3: Corporate Kubernetes Cluster

### Why Deploy to Corporate Cluster?

- **Real production environment** (or close to it)
- **Shared with team** for integration testing
- **Stable infrastructure** managed by ops team
- **CI/CD integration** potential

### The Transition: Key Differences

| Aspect | Local K8s | Corporate K8s |
|--------|-----------|---------------|
| **Images** | Built locally | Must be in registry |
| **Architecture** | ARM (Mac) or AMD64 | AMD64 Linux |
| **Registry** | Docker Desktop | nexus.telenity.com |
| **Image Tags** | `latest` or `1.0.0.1` | `1.0.0.1-amd64` |
| **Access** | localhost | Corporate hostname |
| **Cluster** | Docker Desktop | Real cluster |
| **kubectl config** | default context | Need correct context |

### The Migration Process

#### Step 1: Prepare Images for AMD64

If you're on a Mac (ARM architecture), you need to build for AMD64:

```bash
# Old way (local)
docker build -t stc-vcp/httpd:latest ./httpd

# New way (cross-platform for corporate)
docker buildx build --platform linux/amd64 \
  -t nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-httpd:1.0.0.1-amd64 \
  -f httpd/Dockerfile.vcp.prod \
  ./httpd
```

**Why AMD64?**
- Corporate Linux servers use AMD64 architecture
- Mac uses ARM64 (Apple Silicon)
- Images must match the server architecture

#### Step 2: Push to Corporate Registry

```bash
# Login to Nexus
docker login nexus.telenity.com

# Push each image
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-httpd:1.0.0.1-amd64
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-a3gw:1.0.0.1-amd64
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-adminportal:1.0.0.1-amd64
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-ccportal:1.0.0.1-amd64
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-partnerportal:1.0.0.1-amd64
```

#### Step 3: Update Values File

**Local (`values-local.yaml`):**
```yaml
a3gw:
  image:
    repository: stc-vcp/a3gw
    tag: latest
  replicaCount: 1
```

**Corporate (`values.yaml`):**
```yaml
a3gw:
  image:
    repository: nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-a3gw
    tag: 1.0.0.1-amd64
  replicaCount: 2  # Production uses 2+ replicas
```

#### Step 4: Configure kubectl for Corporate Cluster

```bash
# List available contexts
kubectl config get-contexts

# Switch to corporate cluster
kubectl config use-context <corporate-context-name>

# Verify you're on the right cluster
kubectl cluster-info

# Check namespaces
kubectl get namespaces
```

#### Step 5: Deploy to Corporate Cluster

```bash
# Create namespace (first time only)
kubectl create namespace stc-vcp-services

# Deploy
helm install consolportals . \
  -f values.yaml \
  -n stc-vcp-services

# Verify
kubectl get pods -n stc-vcp-services
kubectl get services -n stc-vcp-services
kubectl get ingress -n stc-vcp-services
```

### Corporate Development Workflow

#### Making Changes

```bash
# 1. Make code changes
vim vcp-adminportal/src/app/main.js

# 2. Bump version number
# Edit values.yaml: 1.0.0.1-amd64 ‚Üí 1.0.0.2-amd64

# 3. Build AMD64 image
docker buildx build --platform linux/amd64 \
  -t nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-adminportal:1.0.0.2-amd64 \
  -f vcp-adminportal/Dockerfile.prod \
  ./vcp-adminportal

# 4. Push to registry
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-adminportal:1.0.0.2-amd64

# 5. Update Helm deployment
helm upgrade consolportals . -f values.yaml -n stc-vcp-services

# 6. Watch rollout
kubectl rollout status deployment/consolportals-sa-stc-vcp-adminportal-deployment -n stc-vcp-services
```

#### Debugging in Corporate

```bash
# View logs
kubectl logs -f deployment/consolportals-sa-stc-vcp-adminportal-deployment -n stc-vcp-services

# Describe pod
kubectl describe pod <pod-name> -n stc-vcp-services

# Check events
kubectl get events -n stc-vcp-services --sort-by='.lastTimestamp'

# Port-forward for direct access (debugging)
kubectl port-forward svc/consolportals-sa-stc-vcp-httpd-service 9080:80 -n stc-vcp-services
```

### Key Files at This Stage

- `values.yaml` - Production Helm configuration (THIS is what's used)
- `values-dev.yaml` - Development environment variant (if needed)
- Same `templates/*.yaml` as local (no changes needed!)
- `.dockerignore` - What to exclude from images

### Best Practices for Corporate Deployment

1. **Version Everything**
   ```bash
   # Always increment versions
   1.0.0.1-amd64 ‚Üí 1.0.0.2-amd64 ‚Üí 1.0.0.3-amd64
   ```

2. **Test Locally First**
   ```bash
   # Always test on local K8s before corporate
   helm install consolportals . -f values-local.yaml -n test
   # If works, then deploy to corporate
   ```

3. **Use Descriptive Commit Messages**
   ```bash
   git commit -m "feat(adminportal): add user export feature [v1.0.0.2]"
   ```

4. **Tag Git Releases**
   ```bash
   git tag -a v1.0.0.2 -m "Release version 1.0.0.2"
   git push origin v1.0.0.2
   ```

5. **Document Changes**
   ```bash
   # Keep a CHANGELOG.md
   ## [1.0.0.2] - 2026-01-28
   ### Added
   - User export functionality in admin portal
   ### Fixed
   - Session timeout issue in a3gw
   ```

---

## Quick Reference: All Three Stages

### Stage 1: Docker Compose

```bash
# Start
make start dev

# Access
http://localhost/adminportal

# Logs
make logs dev

# Stop
make stop dev
```

### Stage 2: Local Kubernetes

```bash
# Start (NodePort - recommended)
helm install consolportals . -f values-local-nodeport.yaml -n stc-vcp-services

# Access
http://localhost:30080/adminportal

# Logs
kubectl logs -f deployment/consolportals-sa-stc-vcp-httpd-deployment -n stc-vcp-services

# Stop
helm uninstall consolportals -n stc-vcp-services
```

### Stage 3: Corporate Kubernetes

```bash
# Build & push
docker buildx build --platform linux/amd64 -t nexus.telenity.com/.../httpd:1.0.0.1-amd64 ./httpd
docker push nexus.telenity.com/.../httpd:1.0.0.1-amd64

# Deploy
helm install consolportals . -f values.yaml -n stc-vcp-services

# Access
https://consolportals.internal.telenity.com/adminportal

# Logs
kubectl logs -f deployment/consolportals-sa-stc-vcp-httpd-deployment -n stc-vcp-services

# Update
helm upgrade consolportals . -f values.yaml -n stc-vcp-services
```

---

## Complete Example: Feature Development Across All Stages

Let's say you need to add a new feature to the admin portal.

### Stage 1: Develop Locally (Docker Compose)

```bash
# 1. Start development environment
make start dev

# 2. Make your changes
vim vcp-adminportal/src/app/users/export.js

# 3. Changes appear immediately (hot-reload)
# Test at http://localhost/adminportal

# 4. Commit when done
git add .
git commit -m "feat(adminportal): add user export"
```

### Stage 2: Test on Local Kubernetes

```bash
# 1. Stop Docker Compose
make stop dev

# 2. Build production images
docker compose -f docker-compose_prod.yml build

# 3. Deploy to local K8s
helm install consolportals . -f values-local-nodeport.yaml -n stc-vcp-services

# 4. Test at http://localhost:30080/adminportal

# 5. If issues found, iterate:
docker compose -f docker-compose_prod.yml build vcpadminportal
helm upgrade consolportals . -f values-local-nodeport.yaml -n stc-vcp-services
```

### Stage 3: Deploy to Corporate Kubernetes

```bash
# 1. Build AMD64 images
docker buildx build --platform linux/amd64 \
  -t nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-adminportal:1.0.0.2-amd64 \
  -f vcp-adminportal/Dockerfile.prod \
  ./vcp-adminportal

# 2. Push to registry
docker push nexus.telenity.com/com/telenity/consolportals-sa-stc-vcp-adminportal:1.0.0.2-amd64

# 3. Update values.yaml with new version
vim values.yaml  # Change 1.0.0.1-amd64 to 1.0.0.2-amd64

# 4. Deploy
kubectl config use-context <corporate-context>
helm upgrade consolportals . -f values.yaml -n stc-vcp-services

# 5. Verify
kubectl get pods -n stc-vcp-services
kubectl logs -f deployment/consolportals-sa-stc-vcp-adminportal-deployment -n stc-vcp-services

# 6. Test at https://consolportals.internal.telenity.com/adminportal
```

---

## Tips for Success

### Stage 1 (Docker)
- ‚úÖ Use `make` commands for convenience
- ‚úÖ Keep `docker-compose_dev.yml` for development
- ‚úÖ Use `docker-compose_prod.yml` before moving to K8s

### Stage 2 (Local K8s)
- ‚úÖ Start with NodePort (easiest)
- ‚úÖ Learn kubectl commands
- ‚úÖ Practice with Helm upgrades
- ‚úÖ Test configurations before corporate deployment

### Stage 3 (Corporate K8s)
- ‚úÖ Always test locally first
- ‚úÖ Use semantic versioning
- ‚úÖ Document all changes
- ‚úÖ Tag Git releases
- ‚úÖ Coordinate with team for deployments

### Overall
- ‚úÖ Commit often, push when stable
- ‚úÖ Write good commit messages
- ‚úÖ Keep documentation updated
- ‚úÖ Ask for help when stuck!

---

**Ready to start?** Head to [GETTING-STARTED.md](2.%20GETTING-STARTED.md) and begin with Stage 1!
